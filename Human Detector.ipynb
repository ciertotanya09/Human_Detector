{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install opencv numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cv2.startWindowThread()\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    # reading the frame\n",
    "    ret, frame = cap.read()\n",
    "    # displaying the frame\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        # breaking the loop if the user types q\n",
    "        # note that the video window must be highlighted!\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "# the following is necessary on the mac,\n",
    "# maybe not on other platforms:\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import cv2\n",
    " \n",
    "# initialize the HOG descriptor/person detector\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "cv2.startWindowThread()\n",
    "\n",
    "# open webcam video stream\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# the output will be written to output.avi\n",
    "out = cv2.VideoWriter(\n",
    "    'output.avi',\n",
    "    cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "    15.,\n",
    "    (640,480))\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # resizing for faster detection\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "    # using a greyscale picture, also for faster detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # detect people in the image\n",
    "    # returns the bounding boxes for the detected objects\n",
    "    boxes, weights = hog.detectMultiScale(frame, winStride=(8,8) )\n",
    "\n",
    "    boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])\n",
    "\n",
    "    for (xA, yA, xB, yB) in boxes:\n",
    "        # display the detected boxes in the colour picture\n",
    "        cv2.rectangle(frame, (xA, yA), (xB, yB),\n",
    "                          (0, 255, 0), 2)\n",
    "    \n",
    "    # Write the output video \n",
    "    out.write(frame.astype('uint8'))\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "# and release the output\n",
    "out.release()\n",
    "# finally, close the window\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "With these parameters, the detection is done almost in real time on my machine. You will see that the detector works better if the person is not too close to the camera. If the person is close, several overlapping boxes are often shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  cv2\n",
    "import numpy as np\n",
    "\n",
    "def find_marker(image):\n",
    "    # convert the image to grayscale and blur to detect edges\n",
    "\n",
    "    blurred_frame = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    hsv = cv2.cvtColor(blurred_frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    lower_blue = np.array([38, 86, 0])\n",
    "    upper_blue = np.array([121, 255, 255])\n",
    "    mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    con = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    return cv2.minAreaRect(con)\n",
    "\n",
    "def distance_to_camera(knownWidth, focalLength, perWidth):\n",
    "    # compute and return the distance from the image to the camera\n",
    "    return (knownWidth * focalLength) / perWidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib.request\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-f')\n",
    "\n",
    "args = parser.parse_args()\n",
    "#here we put the object 30 cm from the camera to calibraing\n",
    "KNOWN_DISTANCE = 30.0\n",
    "\n",
    "#the width of the object  in cm\n",
    "KNOWN_WIDTH = 3.0\n",
    "#we use a ip webcam for capture video\n",
    "\n",
    "url = args.url\n",
    "print(\"<<<---- taking calibrating Image ---->>>\")\n",
    "time.sleep(2)\n",
    "if args.webcam:\n",
    "    cap =  cv2.VideoCapture(0)\n",
    "    _,c_image = cap.read()\n",
    "\n",
    "\n",
    "else:\n",
    "    imgResp = urllib.request.urlopen(url)\n",
    "    imgNp = np.array(bytearray(imgResp.read()), dtype=np.uint8)\n",
    "    c_image = cv2.imdecode(imgNp, -1)\n",
    "\n",
    "\n",
    "\n",
    "marker = find_marker(c_image)\n",
    "focalLength = (marker[1][0] * KNOWN_DISTANCE) / KNOWN_WIDTH\n",
    "\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "print(\"<<<---- Main program Staring ---->>>\")\n",
    "\n",
    "while True:\n",
    "    if args.webcam:\n",
    "        _,image = cap.read()\n",
    "    else:\n",
    "        imgResp = urllib.request.urlopen(url)\n",
    "        image = np.array(bytearray(imgResp.read()), dtype=np.uint8)\n",
    "    # image = cv2.imdecode(imgNp, -1)\n",
    "\n",
    "\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    marker = find_marker(image)\n",
    "    CM = distance_to_camera(KNOWN_WIDTH, focalLength, marker[1][0])\n",
    "\n",
    "    #print the output\n",
    "    cv2.putText(image, \"%.2fcm\" % CM,\n",
    "                (image.shape[1] - 350, image.shape[0] - 15), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                2.0, (255, 0, 0), 3)\n",
    "    cv2.imshow(\"image\", image)\n",
    "    key = cv2.waitKey(1)\n",
    "    #Press Esc to stop the program\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance as dist\n",
    "from imutils import perspective\n",
    "from imutils import contours\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "\n",
    "def midpoint(ptA, ptB):\n",
    "\treturn ((ptA[0] + ptB[0]) * 0.5, (ptA[1] + ptB[1]) * 0.5)\n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-c\", \"--camera\", type=int, required=True,\n",
    "\thelp=\"indice da camera\")\n",
    "ap.add_argument(\"-w\", \"--width\", type=float, required=True,\n",
    "\thelp=\"width of the left-most object in the image (in inches)\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# intialize the camera recording\n",
    "camera = cv2.VideoCapture(args[\"camera\"])\n",
    "Detectou = False #just an aux variable\n",
    "while(1):\n",
    "\n",
    "\t# load the image, convert it to grayscale, and blur it slightly\n",
    "\tret, image = camera.read()\n",
    "\tgray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\tgray = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "\n",
    "\t# perform edge detection, then perform a dilation + erosion to\n",
    "\t# close gaps in between object edges\n",
    "\tedged = cv2.Canny(gray, 50, 100)\n",
    "\tedged = cv2.dilate(edged, None, iterations=1)\n",
    "\tedged = cv2.erode(edged, None, iterations=1)\n",
    "\n",
    "\t# find contours in the edge map\n",
    "\tcnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL,\n",
    "\t\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "\tcnts = cnts[0] if imutils.is_cv2() else cnts[1]\n",
    "\n",
    "\t# sort the contours from left-to-right and initialize the\n",
    "\t# if no object were recognized after a recognition the program will stop with error\n",
    "\ttry:\n",
    "\t\t(cnts, _) = contours.sort_contours(cnts)\n",
    "\t\tDetectou = True\n",
    "\texcept: \n",
    "\t\tif Detectou: raise Parou_De_Detectar\n",
    "\t\telse: pass\n",
    "\t# 'pixels per metric' calibration variable\n",
    "\tpixelsPerMetric = None\n",
    "\torig = image.copy()\n",
    "\t# loop over the contours individually\n",
    "\tfor c in cnts:\n",
    "\t\t# if the contour is not sufficiently large, ignore it\n",
    "\t\tif cv2.contourArea(c) < 250:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# compute the rotated bounding box of the contour\n",
    "\t\t#orig = image.copy()\n",
    "\t\tbox = cv2.minAreaRect(c)\n",
    "\t\tbox = cv2.cv.BoxPoints(box) if imutils.is_cv2() else cv2.boxPoints(box)\n",
    "\t\tbox = np.array(box, dtype=\"int\")\n",
    "\n",
    "\t\t# order the points in the contour such that they appear\n",
    "\t\t# in top-left, top-right, bottom-right, and bottom-left\n",
    "\t\t# order, then draw the outline of the rotated bounding\n",
    "\t\t# box\n",
    "\t\tbox = perspective.order_points(box)\n",
    "\t\tcv2.drawContours(orig, [box.astype(\"int\")], -1, (0, 255, 0), 2)\n",
    "\n",
    "\t\t# loop over the original points and draw them\n",
    "\t\tfor (x, y) in box:\n",
    "\t\t\tcv2.circle(orig, (int(x), int(y)), 5, (0, 0, 255), -1)\n",
    "\n",
    "\t\t# unpack the ordered bounding box, then compute the midpoint\n",
    "\t\t# between the top-left and top-right coordinates, followed by\n",
    "\t\t# the midpoint between bottom-left and bottom-right coordinates\n",
    "\t\t(tl, tr, br, bl) = box\n",
    "\t\t(tltrX, tltrY) = midpoint(tl, tr)\n",
    "\t\t(blbrX, blbrY) = midpoint(bl, br)\n",
    "\n",
    "\t\t# compute the midpoint between the top-left and top-right points,\n",
    "\t\t# followed by the midpoint between the top-righ and bottom-right\n",
    "\t\t(tlblX, tlblY) = midpoint(tl, bl)\n",
    "\t\t(trbrX, trbrY) = midpoint(tr, br)\n",
    "\n",
    "\t\t# draw the midpoints on the image\n",
    "\t\tcv2.circle(orig, (int(tltrX), int(tltrY)), 5, (255, 0, 0), -1)\n",
    "\t\tcv2.circle(orig, (int(blbrX), int(blbrY)), 5, (255, 0, 0), -1)\n",
    "\t\tcv2.circle(orig, (int(tlblX), int(tlblY)), 5, (255, 0, 0), -1)\n",
    "\t\tcv2.circle(orig, (int(trbrX), int(trbrY)), 5, (255, 0, 0), -1)\n",
    "\n",
    "\t\t# draw lines between the midpoints\n",
    "\t\tcv2.line(orig, (int(tltrX), int(tltrY)), (int(blbrX), int(blbrY)),\n",
    "\t\t\t(255, 0, 255), 2)\n",
    "\t\tcv2.line(orig, (int(tlblX), int(tlblY)), (int(trbrX), int(trbrY)),\n",
    "\t\t\t(255, 0, 255), 2)\n",
    "\n",
    "\t\t# compute the Euclidean distance between the midpoints\n",
    "\t\tdA = dist.euclidean((tltrX, tltrY), (blbrX, blbrY))\n",
    "\t\tdB = dist.euclidean((tlblX, tlblY), (trbrX, trbrY))\n",
    "\n",
    "\t\t# if the pixels per metric has not been initialized, then\n",
    "\t\t# compute it as the ratio of pixels to supplied metric\n",
    "\t\t# (in this case, inches)\n",
    "\t\tif pixelsPerMetric is None:\n",
    "\t\t\tpixelsPerMetric = dB / args[\"width\"]\n",
    "\n",
    "\t\t# compute the size of the object\n",
    "\t\tdimA = dA / pixelsPerMetric\n",
    "\t\tdimB = dB / pixelsPerMetric\n",
    "\n",
    "\t\t# draw the object sizes on the image\n",
    "\t\tcv2.putText(orig, \"{:.1f}cm\".format(dimA),\n",
    "\t\t\t(int(tltrX - 15), int(tltrY - 10)), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "\t\t\t0.65, (255, 255, 255), 2)\n",
    "\t\tcv2.putText(orig, \"{:.1f}cm\".format(dimB),\n",
    "\t\t\t(int(trbrX + 10), int(trbrY)), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "\t\t\t0.65, (255, 255, 255), 2)\n",
    "\n",
    "\t\t# show the output image\n",
    "\tcv2.imshow(\"Image\", orig)\n",
    "\tcv2.waitKey(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
